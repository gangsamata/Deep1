{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 05 - Implement the Continuous Bag of Words (CBOW) Model"
      ],
      "metadata": {
        "id": "hF5RR4Vbyv0V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D3-2eL9acQen"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuuXqO3yMBSi",
        "outputId": "74be2640-4fb9-4e38-93c9-0c022281d85f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9b9c65e350>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
        "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "print(hello_embed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_4fi-mmO4h_",
        "outputId": "f6d3ac6a-baec-43b0-de5a-58e4083c1a53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2\n",
        "EMBEDDING_DIM = 10\n",
        "# We will use Shakespeare Sonnet 2\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
      ],
      "metadata": {
        "id": "L2gp8RgPO9uk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAQGO4HkPKuu",
        "outputId": "5ba6fc83-1faf-4b2c-8b43-1c43b90660c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['When',\n",
              " 'forty',\n",
              " 'winters',\n",
              " 'shall',\n",
              " 'besiege',\n",
              " 'thy',\n",
              " 'brow,',\n",
              " 'And',\n",
              " 'dig',\n",
              " 'deep',\n",
              " 'trenches',\n",
              " 'in',\n",
              " 'thy',\n",
              " \"beauty's\",\n",
              " 'field,',\n",
              " 'Thy',\n",
              " \"youth's\",\n",
              " 'proud',\n",
              " 'livery',\n",
              " 'so',\n",
              " 'gazed',\n",
              " 'on',\n",
              " 'now,',\n",
              " 'Will',\n",
              " 'be',\n",
              " 'a',\n",
              " \"totter'd\",\n",
              " 'weed',\n",
              " 'of',\n",
              " 'small',\n",
              " 'worth',\n",
              " 'held:',\n",
              " 'Then',\n",
              " 'being',\n",
              " 'asked,',\n",
              " 'where',\n",
              " 'all',\n",
              " 'thy',\n",
              " 'beauty',\n",
              " 'lies,',\n",
              " 'Where',\n",
              " 'all',\n",
              " 'the',\n",
              " 'treasure',\n",
              " 'of',\n",
              " 'thy',\n",
              " 'lusty',\n",
              " 'days;',\n",
              " 'To',\n",
              " 'say,',\n",
              " 'within',\n",
              " 'thine',\n",
              " 'own',\n",
              " 'deep',\n",
              " 'sunken',\n",
              " 'eyes,',\n",
              " 'Were',\n",
              " 'an',\n",
              " 'all-eating',\n",
              " 'shame,',\n",
              " 'and',\n",
              " 'thriftless',\n",
              " 'praise.',\n",
              " 'How',\n",
              " 'much',\n",
              " 'more',\n",
              " 'praise',\n",
              " \"deserv'd\",\n",
              " 'thy',\n",
              " \"beauty's\",\n",
              " 'use,',\n",
              " 'If',\n",
              " 'thou',\n",
              " 'couldst',\n",
              " 'answer',\n",
              " \"'This\",\n",
              " 'fair',\n",
              " 'child',\n",
              " 'of',\n",
              " 'mine',\n",
              " 'Shall',\n",
              " 'sum',\n",
              " 'my',\n",
              " 'count,',\n",
              " 'and',\n",
              " 'make',\n",
              " 'my',\n",
              " 'old',\n",
              " \"excuse,'\",\n",
              " 'Proving',\n",
              " 'his',\n",
              " 'beauty',\n",
              " 'by',\n",
              " 'succession',\n",
              " 'thine!',\n",
              " 'This',\n",
              " 'were',\n",
              " 'to',\n",
              " 'be',\n",
              " 'new',\n",
              " 'made',\n",
              " 'when',\n",
              " 'thou',\n",
              " 'art',\n",
              " 'old,',\n",
              " 'And',\n",
              " 'see',\n",
              " 'thy',\n",
              " 'blood',\n",
              " 'warm',\n",
              " 'when',\n",
              " 'thou',\n",
              " \"feel'st\",\n",
              " 'it',\n",
              " 'cold.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(test_sentence)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGJvY1bOP1Ts",
        "outputId": "f78f8afc-892e-4138-d116-551e5a508d22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'This\",\n",
              " 'And',\n",
              " 'How',\n",
              " 'If',\n",
              " 'Proving',\n",
              " 'Shall',\n",
              " 'Then',\n",
              " 'This',\n",
              " 'Thy',\n",
              " 'To',\n",
              " 'Were',\n",
              " 'When',\n",
              " 'Where',\n",
              " 'Will',\n",
              " 'a',\n",
              " 'all',\n",
              " 'all-eating',\n",
              " 'an',\n",
              " 'and',\n",
              " 'answer',\n",
              " 'art',\n",
              " 'asked,',\n",
              " 'be',\n",
              " 'beauty',\n",
              " \"beauty's\",\n",
              " 'being',\n",
              " 'besiege',\n",
              " 'blood',\n",
              " 'brow,',\n",
              " 'by',\n",
              " 'child',\n",
              " 'cold.',\n",
              " 'couldst',\n",
              " 'count,',\n",
              " 'days;',\n",
              " 'deep',\n",
              " \"deserv'd\",\n",
              " 'dig',\n",
              " \"excuse,'\",\n",
              " 'eyes,',\n",
              " 'fair',\n",
              " \"feel'st\",\n",
              " 'field,',\n",
              " 'forty',\n",
              " 'gazed',\n",
              " 'held:',\n",
              " 'his',\n",
              " 'in',\n",
              " 'it',\n",
              " 'lies,',\n",
              " 'livery',\n",
              " 'lusty',\n",
              " 'made',\n",
              " 'make',\n",
              " 'mine',\n",
              " 'more',\n",
              " 'much',\n",
              " 'my',\n",
              " 'new',\n",
              " 'now,',\n",
              " 'of',\n",
              " 'old',\n",
              " 'old,',\n",
              " 'on',\n",
              " 'own',\n",
              " 'praise',\n",
              " 'praise.',\n",
              " 'proud',\n",
              " 'say,',\n",
              " 'see',\n",
              " 'shall',\n",
              " 'shame,',\n",
              " 'small',\n",
              " 'so',\n",
              " 'succession',\n",
              " 'sum',\n",
              " 'sunken',\n",
              " 'the',\n",
              " 'thine',\n",
              " 'thine!',\n",
              " 'thou',\n",
              " 'thriftless',\n",
              " 'thy',\n",
              " 'to',\n",
              " \"totter'd\",\n",
              " 'treasure',\n",
              " 'trenches',\n",
              " 'use,',\n",
              " 'warm',\n",
              " 'weed',\n",
              " 'were',\n",
              " 'when',\n",
              " 'where',\n",
              " 'winters',\n",
              " 'within',\n",
              " 'worth',\n",
              " \"youth's\"}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_ix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plBbk0JcP7Lo",
        "outputId": "3280fb8b-902c-44cf-eaae-b9174a119d7a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'so': 0,\n",
              " 'own': 1,\n",
              " 'mine': 2,\n",
              " 'praise': 3,\n",
              " 'lies,': 4,\n",
              " 'small': 5,\n",
              " 'where': 6,\n",
              " 'thy': 7,\n",
              " \"'This\": 8,\n",
              " 'besiege': 9,\n",
              " 'now,': 10,\n",
              " 'thou': 11,\n",
              " 'and': 12,\n",
              " 'sunken': 13,\n",
              " 'eyes,': 14,\n",
              " 'dig': 15,\n",
              " 'sum': 16,\n",
              " 'being': 17,\n",
              " 'it': 18,\n",
              " 'field,': 19,\n",
              " 'when': 20,\n",
              " 'blood': 21,\n",
              " 'Were': 22,\n",
              " 'fair': 23,\n",
              " 'all': 24,\n",
              " 'an': 25,\n",
              " \"deserv'd\": 26,\n",
              " 'of': 27,\n",
              " \"youth's\": 28,\n",
              " 'succession': 29,\n",
              " 'old': 30,\n",
              " 'Shall': 31,\n",
              " 'thine!': 32,\n",
              " 'worth': 33,\n",
              " 'new': 34,\n",
              " 'livery': 35,\n",
              " 'This': 36,\n",
              " \"excuse,'\": 37,\n",
              " 'much': 38,\n",
              " 'Will': 39,\n",
              " 'old,': 40,\n",
              " 'his': 41,\n",
              " 'asked,': 42,\n",
              " 'forty': 43,\n",
              " \"totter'd\": 44,\n",
              " 'within': 45,\n",
              " 'count,': 46,\n",
              " 'be': 47,\n",
              " 'To': 48,\n",
              " 'Where': 49,\n",
              " 'use,': 50,\n",
              " 'answer': 51,\n",
              " 'praise.': 52,\n",
              " 'Thy': 53,\n",
              " 'And': 54,\n",
              " 'Then': 55,\n",
              " 'my': 56,\n",
              " 'proud': 57,\n",
              " 'gazed': 58,\n",
              " 'If': 59,\n",
              " 'held:': 60,\n",
              " 'make': 61,\n",
              " 'treasure': 62,\n",
              " 'by': 63,\n",
              " 'all-eating': 64,\n",
              " 'in': 65,\n",
              " 'to': 66,\n",
              " 'thriftless': 67,\n",
              " 'more': 68,\n",
              " 'thine': 69,\n",
              " 'a': 70,\n",
              " 'warm': 71,\n",
              " 'see': 72,\n",
              " 'say,': 73,\n",
              " 'deep': 74,\n",
              " \"beauty's\": 75,\n",
              " 'cold.': 76,\n",
              " 'on': 77,\n",
              " 'brow,': 78,\n",
              " 'days;': 79,\n",
              " \"feel'st\": 80,\n",
              " 'lusty': 81,\n",
              " 'winters': 82,\n",
              " 'shame,': 83,\n",
              " 'art': 84,\n",
              " 'When': 85,\n",
              " 'beauty': 86,\n",
              " 'weed': 87,\n",
              " 'couldst': 88,\n",
              " 'the': 89,\n",
              " 'Proving': 90,\n",
              " 'trenches': 91,\n",
              " 'made': 92,\n",
              " 'child': 93,\n",
              " 'How': 94,\n",
              " 'shall': 95,\n",
              " 'were': 96}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ],
      "metadata": {
        "id": "K4-OOj1eQkcx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2\n",
        "EMDEDDING_DIM = 100\n",
        "\n",
        "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\".split()"
      ],
      "metadata": {
        "id": "WLzk2mb1Qn-e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
        "ix_to_word = {ix:word for ix, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "_VUI_B8HQtVV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for i in range(2, len(raw_text) - 2):\n",
        "    context = [raw_text[i - 2], raw_text[i - 1],\n",
        "               raw_text[i + 1], raw_text[i + 2]]\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "data[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpAgk4YxQvxl",
        "outputId": "081cd5af-b6e5-4f4a-c872-7cdcaf21ee95"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['We', 'are', 'to', 'study'], 'about')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "\n",
        "        #out: 1 x emdedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        self.activation_function1 = nn.ReLU()\n",
        "        \n",
        "        #out: 1 x vocab_size\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
        "        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.activation_function1(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.activation_function2(out)\n",
        "        return out\n",
        "\n",
        "    def get_word_emdedding(self, word):\n",
        "        word = torch.tensor([word_to_ix[word]])\n",
        "        return self.embeddings(word).view(1,-1)"
      ],
      "metadata": {
        "id": "DCy_axt8Q2bO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CBOW(vocab_size, EMDEDDING_DIM)\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "jw22q1FkQ5B-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "\n",
        "    for context, target in data:\n",
        "        context_vector = make_context_vector(context, word_to_ix)  \n",
        "\n",
        "        log_probs = model(context_vector)\n",
        "\n",
        "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
        "\n",
        "    #optimize at the end of each epoch\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "P5tQITwMQ-m5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTING\n",
        "context = ['People','create','to', 'direct']\n",
        "context_vector = make_context_vector(context, word_to_ix)\n",
        "a = model(context_vector)"
      ],
      "metadata": {
        "id": "k6sDbYoBRCB3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print result\n",
        "print(f'Raw text: {\" \".join(raw_text)}\\n')\n",
        "print(f'Context: {context}\\n')\n",
        "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pCi6C5fRGV6",
        "outputId": "560bec42-f484-4d3f-ab90-47f388104a64"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw text: We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.\n",
            "\n",
            "Context: ['People', 'create', 'to', 'direct']\n",
            "\n",
            "Prediction: programs\n"
          ]
        }
      ]
    }
  ]
}